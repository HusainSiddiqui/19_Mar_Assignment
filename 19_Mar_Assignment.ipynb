{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique that scales numerical features to a fixed range of values, usually between 0 and 1. \n",
    "This technique is also known as normalization, as it transforms the data to a common scale without changing the distribution of the data.\n",
    "\n",
    "x_scaled=x-min(x)/max(x)-min(x)\n",
    "\n",
    "For example, suppose we have a dataset of house prices, and we want to scale the 'price' feature using Min-Max scaling. \n",
    "The minimum and maximum prices in the dataset are  $100,000 and  $500,000, respectively. If we have a house with a price of $300,000, we can scale it as follows:\n",
    "\n",
    "x_scaled = {300,000 - 100,000}/{500,000 - 100,000} = 0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unit Vector technique in feature scaling is a method to rescale the features in a dataset so that each feature has a unit length. It is also known as normalization, and it involves dividing each observation by its magnitude. \n",
    "\n",
    "Unlike Min-Max scaling, which scales the features to a specific range (usually between 0 and 1), the Unit Vector technique scales the features based on their magnitude. This means that the scale of the features is dependent on the variability of the data. \n",
    "\n",
    "For example, suppose we have a dataset with two features, age (ranging from 0 to 100) and income (ranging from 0 to 100,000). To apply the Unit Vector technique, we first calculate the magnitude of each observation as the square root of the sum of the squares of the features:\n",
    "\n",
    "magnitude = sqrt(age^2 + income^2)\n",
    "\n",
    "Then, we divide each feature by its corresponding magnitude to get the scaled values:\n",
    "\n",
    "age_scaled = age / magnitude\n",
    "income_scaled = income / magnitude\n",
    "\n",
    "The resulting scaled values will have a length of 1, which means they lie on the unit circle. This technique is useful when the scale of the features is unknown or varies widely, and we want to ensure that each feature has the same weight in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA, or Principal Component Analysis, is a statistical technique used in machine learning for reducing the dimensionality of a dataset by transforming the original features into a smaller set of orthogonal components called principal components.\n",
    "\n",
    "PCA works by identifying the directions in the data that contain the most variance, or the most information, and then projecting the data onto those directions. The first principal component is the direction in the data that contains the most variance, and each subsequent principal component is orthogonal to the previous ones and contains the most remaining variance.\n",
    "\n",
    "PCA can be used for dimensionality reduction because it allows us to represent a high-dimensional dataset in a lower-dimensional space while preserving the most important information about the data. This can be useful for reducing noise, speeding up machine learning algorithms, and visualizing high-dimensional data.\n",
    "\n",
    "For example, let's say we have a dataset with 100 features. We could use PCA to reduce the dimensionality of the dataset to 10 principal components, which would capture the most important information about the data while reducing the number of features we need to consider. We could then use these 10 principal components as input to a machine learning algorithm instead of the original 100 features, which could improve the performance of the algorithm and reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) and Feature Extraction are closely related concepts in machine learning. In fact, PCA can be used as a feature extraction technique to transform high-dimensional data into a lower-dimensional space, where the new features (principal components) capture the most important patterns or variations in the original data.\n",
    "\n",
    "Here's an example to illustrate the concept of using PCA for feature extraction:\n",
    "\n",
    "Suppose we have a dataset of 1000 images, each with a resolution of 128x128 pixels, resulting in 16384 features per image. We want to reduce the dimensionality of the dataset while retaining as much information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Min-Max scaling to preprocess the data for the food delivery recommendation system, we would first need to apply the scaling to each of the numerical features (price, rating, and delivery time) independently. This involves transforming the values of each feature so that they fall within a specific range, typically between 0 and 1.\n",
    "\n",
    "To do this, we would first calculate the minimum and maximum values for each feature in the dataset. We would then use the following formula to transform the values of each feature:\n",
    "\n",
    "scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "This would ensure that the values for each feature are scaled proportionally, so that their relative importance is preserved. Once the scaling is applied, we can then use the preprocessed data to build our recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with a large number of features, it is often helpful to use PCA (Principle Component Analysis) to reduce the dimensionality of the dataset. This involves finding a smaller set of features that can still explain the majority of the variation in the data. \n",
    "\n",
    "To use PCA for dimensionality reduction in a stock price prediction project, the first step would be to standardize the features to ensure they are on the same scale. This can be done using techniques such as Z-score normalization or Min-Max scaling. \n",
    "\n",
    "Next, PCA would be applied to the standardized dataset to find the principal components that explain the most variation in the data. These principal components would represent a smaller set of features that can be used in the model instead of the original features. \n",
    "\n",
    "The number of principal components to retain can be determined by looking at the explained variance plot and selecting the number of components that explain a significant amount of the variation in the data. \n",
    "\n",
    "By using PCA to reduce the dimensionality of the dataset, the model can be trained on a smaller set of features, which can improve its efficiency and reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.21052632 0.47368421 0.73684211 1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the minimum and maximum values\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "\n",
    "# Perform Min-Max scaling to transform the values to a range of -1 to 1\n",
    "scaled_data = (data - min_val) / (max_val - min_val) \n",
    "\n",
    "# Print the scaled data\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of principal components to retain in PCA depends on the specific dataset and the requirements of the project. One common approach is to choose the number of principal components that explain a certain percentage of the total variance in the data, such as 95% or 99%.\n",
    "\n",
    "To determine the number of principal components to retain, we can perform PCA on the dataset and examine the explained variance ratio for each principal component. The explained variance ratio indicates the proportion of variance in the data that is explained by each principal component. We can then choose the number of principal components that explain a desired percentage of the total variance.\n",
    "\n",
    "In the case of the given dataset, it is not possible to determine the appropriate number of principal components to retain without further information about the dataset and the specific project requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
